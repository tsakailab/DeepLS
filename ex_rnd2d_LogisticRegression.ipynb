{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tsakailab/DeepLS/blob/main/ex_rnd2d_LogisticRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "UTNIg2oEn2n8"
      },
      "cell_type": "markdown",
      "source": [
        "# 2次元データのロジスティック回帰（logistic regression of a 2D dataset）\n",
        "\n",
        "----\n",
        "\n",
        "氏名：\n",
        "\n",
        "学生番号：\n",
        "\n",
        "----\n",
        "基本課題（必須）\n",
        "\n",
        "    1. 「★scikit-learnを用いたロジスティック回帰」まで実行すると，どのような数値と図が出力されますか．\n",
        "       また，表示される数値と図から何が言えますか．特に，Example 2とExample 3について記述してください．\n",
        "\n",
        "（ここに回答を書いてください）\n",
        "\n",
        "\n",
        "\n",
        "    2. 「★scikit-learnを用いたロジスティック回帰」で使用している model.fit(X, y) と\n",
        "        model.decision_function(X) はそれぞれ何をする関数ですか．\n",
        "\n",
        "（ここに回答を書いてください）\n",
        "\n",
        "\n",
        "\n",
        "    3. 最急降下法によるロジスティック回帰を実装してください．\n",
        "\n",
        "（回答はこのファイルの後半「★最急降下法によるロジスティック回帰の実装」に書いてください）\n",
        "\n",
        "\n",
        "\n",
        "    4.その他，気づいたこと，調べたことを書いてください．\n",
        "\n",
        "（ここに回答を書いてください）\n",
        "\n",
        "\n",
        "\n",
        "----\n",
        "発展課題（任意）がこのノートブックの後半にあります．"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title グラフを描くクラス plot2cls を定義します（理解不要）．\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.colors import TwoSlopeNorm as tsn\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")#, category=np.VisibleDeprecationWarning)\n",
        "\n",
        "class plot2cls:\n",
        "    def __init__(self, X_train, y_train, decision_function=None, X_val=None, y_val=None, dx=0.02, cmap=plt.cm.bwr, xlim=None, ylim=None, margin=0.5, levels={0.0:'-'}, colors='k', bins=None):\n",
        "        self.data = {'train': {'X': X_train, 'y': y_train}, 'val': {'X':X_val, 'y': y_val}}\n",
        "        self.clf = decision_function\n",
        "        if X_val is not None:\n",
        "            xlim = [X_val[:, 0].min(), X_val[:, 0].max()]\n",
        "            ylim = [X_val[:, 1].min(), X_val[:, 1].max()]\n",
        "        else:\n",
        "            xlim = [float('inf'), - float('inf')]\n",
        "            ylim = [float('inf'), - float('inf')]\n",
        "        xlim[0] = min(xlim[0], X_train[:, 0].min()) - margin\n",
        "        xlim[1] = max(xlim[1], X_train[:, 0].max()) + margin\n",
        "        ylim[0] = min(ylim[0], X_train[:, 1].min()) - margin\n",
        "        ylim[1] = max(ylim[1], X_train[:, 1].max()) + margin\n",
        "        if bins is None:\n",
        "            bins = len(y_train) // 4\n",
        "        self.layout = {'dx': dx, 'cmap': cmap, 'xlim': xlim, 'ylim': ylim, 'levels': levels, 'colors': colors, 'bins': bins}\n",
        "\n",
        "\n",
        "    def _data_layout(self, ax, bbox_to_anchor, loc):\n",
        "        ax.set_xlim(self.layout['xlim'][0], self.layout['xlim'][1])\n",
        "        ax.set_ylim(self.layout['ylim'][0], self.layout['ylim'][1])\n",
        "        #plt.axis('tight')\n",
        "        ax.set_xlabel('x1', fontsize=16)\n",
        "        ax.set_ylabel('x2', fontsize=16)\n",
        "        ax.tick_params(axis='x', labelsize=16)\n",
        "        ax.tick_params(axis='y', labelsize=16)\n",
        "        ax.set_aspect('equal')\n",
        "        #ax.legend(bbox_to_anchor=(1.5,1.0), loc=\"upper right\", fontsize=16, frameon=True)\n",
        "        ax.legend(bbox_to_anchor=bbox_to_anchor, loc=loc, fontsize=16, frameon=True)\n",
        "        if self.data['val']['X'] is not None and self.data['val']['y'] is not None:\n",
        "            ax.legend(bbox_to_anchor=bbox_to_anchor, loc=loc, fontsize=16, frameon=True, ncol=2)\n",
        "            ax.get_legend().legendHandles[2].set_color('k')\n",
        "            ax.get_legend().legendHandles[3].set_color('k')\n",
        "        plt.tight_layout()\n",
        "\n",
        "\n",
        "    def _scatter(self, ax, X, y, c=['r', 'b'], marker=['s', 'o'], label=None, alpha=1):\n",
        "        ax.scatter(X[y>0, 0], X[y>0, 1], c=c[0],  marker=marker[0], cmap=self.layout['cmap'], edgecolors='k', label=label+'(+)', alpha=alpha)\n",
        "        ax.scatter(X[y<=0, 0], X[y<=0, 1], c=c[1], marker=marker[1], cmap=self.layout['cmap'], edgecolors='k', label=label+'(-)', alpha=alpha)\n",
        "\n",
        "\n",
        "    def _put_data(self, ax, alpha):\n",
        "        if self.clf is not None and self.layout['levels'] is not None:\n",
        "            xx, yy = np.meshgrid(np.arange(self.layout['xlim'][0], self.layout['xlim'][1], self.layout['dx']), np.arange(self.layout['ylim'][0], self.layout['ylim'][1], self.layout['dx']))\n",
        "\n",
        "            # Show prediction by color by assigning a color to each point in the mesh [x_min, x_max]x[y_min, y_max].\n",
        "            Z = self.clf(np.c_[xx.ravel(), yy.ravel()])\n",
        "            # Put the result into a color plot\n",
        "            Z = Z.reshape(xx.shape)\n",
        "            norm = tsn(vmin=np.minimum(Z[:].min(),-1e-6), vcenter=0, vmax=np.maximum(Z[:].max(),1e-6))\n",
        "            if self.layout['cmap'] is not None:\n",
        "                ax.pcolor(xx, yy, Z, cmap=self.layout['cmap'], alpha=0.1, edgecolors=None, norm=norm)\n",
        "            lvls = list(self.layout['levels'].keys())\n",
        "            linestyles = list(self.layout['levels'].values())\n",
        "            ax.contour(xx, yy, Z, levels=lvls, colors=self.layout['colors'], linestyles=linestyles, alpha=0.5)\n",
        "\n",
        "        self._scatter(ax, self.data['train']['X'], self.data['train']['y'], label='train', alpha=alpha)\n",
        "        if self.data['val']['X'] is not None and self.data['val']['y'] is not None:\n",
        "            self._scatter(ax, self.data['val']['X'], self.data['val']['y'], c=['k', 'k'], label='val', alpha=alpha*0.2)\n",
        "\n",
        "\n",
        "    def plot_data(self, bbox_to_anchor=(1,0), loc=\"lower left\"):\n",
        "        ax = plt.figure(figsize=(8,8))\n",
        "        ax = plt.axes()\n",
        "        self._put_data(ax, alpha=1)\n",
        "        self._data_layout(ax, bbox_to_anchor=bbox_to_anchor, loc=loc)\n",
        "\n",
        "\n",
        "    def _hist_layout(self, ax, bbox_to_anchor, loc):\n",
        "        ax.set_xlabel(\"$g(x)$\", fontsize=16)\n",
        "        ax.set_ylabel(\"Frequency\", fontsize=16)\n",
        "        #ax.axis('tight')\n",
        "        ax.tick_params(axis='x', labelsize=16)\n",
        "        ax.tick_params(axis='y', labelsize=16)\n",
        "        #plt.gca().yaxis.set_major_formatter(FormatStrFormatter('%1.0f'))\n",
        "        from matplotlib.ticker import FormatStrFormatter\n",
        "        ax.yaxis.set_major_formatter(FormatStrFormatter('%1.0f'))\n",
        "        #ax.set_aspect(1)\n",
        "        ax.legend(bbox_to_anchor=bbox_to_anchor, loc=loc, fontsize=16, frameon=True)\n",
        "        if self.data['val']['X'] is not None and self.data['val']['y'] is not None:\n",
        "            ax.legend(bbox_to_anchor=bbox_to_anchor, loc=loc, fontsize=16, frameon=True, ncol=2)\n",
        "            #ax.legend(ncol=2)\n",
        "        plt.tight_layout()\n",
        "\n",
        "\n",
        "    def _make_hist(self, ax):\n",
        "        if self.clf is None:\n",
        "            return\n",
        "        pred = self.clf(self.data['train']['X'])\n",
        "        gt = self.data['train']['y']\n",
        "        ax.hist( [ pred[gt>0], pred[gt<=0] ], bins=self.layout['bins'], histtype='stepfilled', density=False, alpha=0.5, color=['r', 'b'], label=['train(+)', 'train(-)'])\n",
        "        if self.data['val']['X'] is not None and self.data['val']['y'] is not None:\n",
        "            pred = self.clf(self.data['val']['X'])\n",
        "            gt = self.data['val']['y']\n",
        "            ax.hist( [ pred[gt>0], pred[gt<=0] ], bins=self.layout['bins'], histtype='stepfilled', density=False, alpha=0.3, color=['r', 'b'], label=['val(+)', 'val(-)'])\n",
        "\n",
        "\n",
        "    def plot_hist(self, loc=\"lower left\", bbox_to_anchor=(0,1)):\n",
        "        ax = plt.figure(figsize=(6,6))\n",
        "        ax = plt.axes()\n",
        "        self._make_hist(ax)\n",
        "        self._hist_layout(ax,bbox_to_anchor, loc)\n",
        "\n",
        "\n",
        "    def plot_clf(self, loc=\"lower left\", bbox_to_anchor=(0,1)):\n",
        "        if self.clf is None:\n",
        "            return\n",
        "\n",
        "        fig, axes = plt.subplots(figsize=(12,6), nrows=1, ncols=2)\n",
        "        #fig, axes = plt.subplots(nrows=1, ncols=2)\n",
        "        ax = axes[0]\n",
        "        self._put_data(ax, alpha=1)\n",
        "        self._data_layout(ax, bbox_to_anchor, loc)\n",
        "\n",
        "        #fig.set_figwidth(12)\n",
        "        #fig.set_figheight(8)\n",
        "        ax = axes[1]\n",
        "        self._make_hist(ax)\n",
        "        self._hist_layout(ax, bbox_to_anchor, loc)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "kkKF2Idqdx_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 実験用のデータ（2次元，2クラス）を生成します．\n",
        "- Example 1～4 からひとつ選んで実行してください．"
      ],
      "metadata": {
        "id": "4byvUzevPKta"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQWiGCU0G9aA"
      },
      "source": [
        "# Example 1: y = or(x1, x2)\n",
        "X = np.array([[0, 0], [1,0], [0,1], [1,1]])\n",
        "y = np.array([-1,1,1,1])\n",
        "\n",
        "X_val = X + np.random.randn(4, 2) * 0.2\n",
        "y_val = y.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPcFoZwoU2fp"
      },
      "source": [
        "# Example 2: draw npos and nneg points from the Gaussian distribution for each class\n",
        "npos = 30\n",
        "nneg = 30\n",
        "np.random.seed(321)\n",
        "X = np.r_[np.random.randn(npos, 2) + [3, 3], np.random.randn(nneg, 2)]\n",
        "# [1,1,...,1,-1,-1,...,-1]\n",
        "y = np.array([1] * npos + [-1] * nneg)\n",
        "\n",
        "X_val = X + np.random.randn(X.shape[0], 2) * 1.0\n",
        "y_val = y.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAcLs8fxU3b8"
      },
      "source": [
        "# Example 3: create moons using sklearn\n",
        "from sklearn.datasets import make_moons\n",
        "X, y = make_moons(n_samples=100, noise=0.2, random_state=0)\n",
        "y[y==0] = -1\n",
        "\n",
        "X_val, y_val = make_moons(n_samples=100, noise=0.2, random_state=1)\n",
        "y_val[y_val==0] = -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5xj700RhOad"
      },
      "source": [
        "# Example 4: create circles using sklearn\n",
        "from sklearn.datasets import make_circles\n",
        "X, y = make_circles(n_samples=150, noise=0.1, random_state=0, factor=0.3)\n",
        "y[y==0] = -1\n",
        "\n",
        "X_val, y_val = make_circles(n_samples=150, noise=0.1, random_state=1, factor=0.3)\n",
        "y_val[y_val==0] = -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title データを表示します．\n",
        "plot2cls(X, y, X_val=X_val, y_val=y_val).plot_data()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "dCj3rKLVdyQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ★scikit-learn を用いたロジスティック回帰\n",
        "\n",
        "[sklearn.linear_model.LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)を使用した結果を出力します．"
      ],
      "metadata": {
        "id": "ssqJLogmRkli"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression()  #(penalty='none')\n",
        "model.fit(X, y)\n",
        "\n",
        "# print([w0], [w1, w2])\n",
        "print(\"w0 =\", model.intercept_, \",  w =\", model.coef_.ravel())\n",
        "\n",
        "print(\"Accuracy on training data: \", model.score(X, y))\n",
        "print(\"Accuracy on validation data: \", model.score(X_val, y_val))\n",
        "\n",
        "dec = lambda X: model.decision_function(X)\n",
        "plot2cls(X, y, dec, X_val, y_val).plot_clf()"
      ],
      "metadata": {
        "id": "_PE0o7dTdyS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 最急降下法によるロジスティック回帰の学習アルゴリズム\n",
        "参考資料 [■](https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html) [■](https://math.stackexchange.com/questions/78575/derivative-of-sigmoid-function-sigma-x-frac11e-x)\n",
        "\n",
        "適当な初期値から，学習率 $\\alpha>0$ で $w_0$ と $\\boldsymbol w$ の修正を次式のように反復します．\n",
        "\n",
        "$\\qquad w_0\\gets w_0 - \\alpha{\\mathit\\Delta}w_0$  \n",
        "$\\qquad \\boldsymbol w\\gets \\boldsymbol w - \\alpha{\\mathit\\Delta}\\boldsymbol w$\n",
        "\n",
        "修正量 ${\\mathit\\Delta}w_0$ と ${\\mathit\\Delta}\\boldsymbol w$ は平均ロジスティック損失 $L(w_0,\\boldsymbol w)$ の勾配で，次式のように表せます．\n",
        "\n",
        "${\\mathit\\Delta}w_0=\\;\\;\\;\\frac{\\partial L(w_0,\\boldsymbol w)}{\\partial w_0}\\;\\; = \\frac{1}{n}\\sum_{j=1}^n \\varepsilon^{(j)}$  \n",
        "\n",
        "${\\mathit\\Delta}\\boldsymbol w=\\left[\\matrix{\\frac{\\partial L(w_0,\\boldsymbol w)}{\\partial w_1}\\\\ \\vdots \\\\\\frac{\\partial L(w_0,\\boldsymbol w)}{\\partial w_d}}\\right] = \\frac{1}{n}\\sum_{j=1}^n \\varepsilon^{(j)}\\boldsymbol x^{(j)}$  \n",
        "\n",
        "ただし，\n",
        "$\\varepsilon^{(j)}=\\left\\{\\begin{array}{cl}p^{(j)}-1 & (y^{(j)}=+1)\\\\ p^{(j)} & (y^{(j)}=-1)\\end{array}\\right.\\qquad$  （確率 $p^{(j)}$ を正解 $y^{(j)}$ と比較した誤差）  \n",
        "\n",
        "$[p^{(1)},\\dots,p^{(n)}]=[\\mathrm{sigmoid}(g(\\boldsymbol x^{(1)})),\\dots,\\mathrm{sigmoid}(g(\\boldsymbol x^{(n)}))]$  \n",
        "\n",
        "です．つまり，データ $\\boldsymbol x^{(j)}$ に対する識別関数の出力を確率 $p^{(j)}$ に換算し，\n",
        "- 正解が $y^{(j)}=+1$ ならば $p^{(j)}$ が $1$ に近いほどよい（$\\varepsilon^{(j)}=p^{(j)}-1$）\n",
        "- 正解が $y^{(j)}=-1$ ならば $p^{(j)}$ が $0$ に近いほどよい（$\\varepsilon^{(j)}=p^{(j)}$）\n",
        "\n",
        "として誤差 $\\varepsilon^{(j)}$ が計算されています．誤差が大きいほど修正量に加味されます．"
      ],
      "metadata": {
        "id": "XhFha4t1wYhG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ★最急降下法によるロジスティック回帰の実装\n",
        "\n",
        "LogisticRegressionGDという名のクラスとして実装しましょう．[sklearn.linear_model.LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) に似た仕様とします．\n",
        "\n",
        "訓練データ `X_train` とクラスラベル `y_train` を学習したあと，`X_val` のクラスラベルの予測 `y_pred` を得たり，正答率を表示するには，\n",
        "```\n",
        "    myclf = LogisticRegressionGD(lr=1e-2, max_iter=300)\n",
        "    myclf.fit(X_train, y_train)\n",
        "    y_pred = myclf.predict(X_val)\n",
        "    print(myclf.score(X_val, y_val))\n",
        "```\n",
        "のように使うことを想定します．ここで，`lr` は学習率（learning rate），`max_iter` は反復回数（maximum number of iterations）です．\n",
        "訓練データの次元数`d`，個数`n`の場合，`X_train` と `y_train` はそれぞれ shape が `(n,d)` と `(n,)` のNumPy配列です．ただし，`y_train` は2クラスを +1と-1で表すものとします．"
      ],
      "metadata": {
        "id": "UBYXMS7ICxHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib.cbook import deprecation\n",
        "import numpy as np\n",
        "\n",
        "class LogisticRegressionGD:\n",
        "\n",
        "    def __init__(self, lr=1., max_iter=1000):\n",
        "        self.lr = lr\n",
        "        self.max_iter = max_iter\n",
        "        self.intercept_ = None                          # w0\n",
        "        self.coef_ = None                               # [w1,...,wd}\n",
        "        self.loss_history = []                          # 損失の履歴\n",
        "\n",
        "\n",
        "    # 識別関数 g(x) = w0 + w1*x1 + ... + wd*xd\n",
        "    def decision_function(self, X):                     # X(n,d)\n",
        "        return ''' self.intercept_ と X.dot(self.coef_) を使う '''\n",
        "\n",
        "    # シグモイド関数 sigmoid(z) = 1/ (1 + exp(-z))\n",
        "    def _sigmoid(self, z):                              # z(n,)\n",
        "        return ''' np.exp() を使う '''\n",
        "\n",
        "    # 平均ロジスティック損失 (1/n)Σ log(1 + exp(-y*g(x)) )\n",
        "    def ave_loss(self, X, y):                           # X(n,d), y(n,)\n",
        "        loss = ''' self.decision_function(), np.log(), len(y) を使う '''\n",
        "        return loss\n",
        "\n",
        "\n",
        "    # 学習：最急降下法で w0,...,wdを最適化します\n",
        "    def fit(self, X, y):                                # X(n,d), y(n,)\n",
        "        n, d = X.shape\n",
        "\n",
        "        # w0 と [w1,...,wd] の初期化\n",
        "        if self.intercept_ is None: self.intercept_ = 0\n",
        "        if self.coef_ is None: self.coef_ = np.zeros(d)\n",
        "\n",
        "        # 勾配による修正の反復\n",
        "        for _ in range(self.max_iter):\n",
        "            p = ''' self.decision_function() と self._sigmoid() を使う．'''\n",
        "            e = np.where(y > 0, p - 1, p)               # e(n,)\n",
        "\n",
        "            dw0 = e.sum() / n\n",
        "            dw = e.dot(X) / n\n",
        "\n",
        "            self.intercept_ -= ''' self.lr と dw0 を使う '''\n",
        "            self.coef_ -= ''' self.lr と dw を使う '''\n",
        "\n",
        "            self.loss_history.append( self.ave_loss(X, y) )\n",
        "\n",
        "\n",
        "    # 推論\n",
        "    def predict(self, X):                               # X(n,d)\n",
        "        y_pred = self.decision_function(X)              # y_pred(n,)\n",
        "        return np.where(y_pred > 0, 1, -1)              # (n,)\n",
        "\n",
        "    def score(self, X, y):\n",
        "        y_pred = self.predict(X)\n",
        "        return (y == y_pred).sum() / len(y)"
      ],
      "metadata": {
        "id": "0wgc_5GpGjyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 実装した`LogisticRegressionGD`で識別します．"
      ],
      "metadata": {
        "id": "5Bk1HovpADsj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegressionGD()  # lr=100.\n",
        "model.fit(X, y)\n",
        "\n",
        "# print([w0], [w1, w2])\n",
        "print(\"w0 =\", model.intercept_, \",  w =\", model.coef_.ravel())\n",
        "\n",
        "print(\"Accuracy on training data: \", model.score(X, y))\n",
        "print(\"Accuracy on validation data: \", model.score(X_val, y_val))\n",
        "\n",
        "plt.plot(model.loss_history)\n",
        "\n",
        "dec = lambda X: model.decision_function(X)\n",
        "plot2cls(X, y, dec, X_val, y_val).plot_clf()"
      ],
      "metadata": {
        "id": "C95O9uF8MmE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------\n",
        "## 発展課題（任意）\n",
        "\n",
        "学習率や反復回数を大きくした場合を考えましょう．  \n",
        "最急降下法によるロジスティック回帰では，同じ問題設定でも scikit-learn の場合と決定境界が異なっていることを確認してください．特に，学習率や反復回数を大きくするほど，求まる重み（`w0` や `[w1,...,wd]`）の値が大きくなります．\n",
        "\n",
        "    1. 重みの大きさと，平均ロジスティック損失の関係を説明してください．\n",
        "\n",
        "（ここに回答を書いてください）\n",
        "\n",
        "\n",
        "ヒント：平均ロジスティック損失 $L(w_0,\\boldsymbol w)=\\frac{1}{n}\\sum_{j=1}^n\\log\\left(1+e^{-y^{(j)}(w_0+\\boldsymbol w^\\top\\boldsymbol x^{(j)})}\\right)$\n",
        "\n",
        "    2. 「★最急降下法によるロジスティック回帰の実装」を変更してL2正則化を導入し，\n",
        "       scikit-learn と同じ結果を再現してください．\n",
        "\n",
        "（コードに変更を施した行の変更前後をここに書いてください）\n",
        "\n",
        "\n",
        "ヒント：\n",
        "- L2正則化項を付加した最小化問題： $\\mathrm{Minimize}_{(w_0,\\boldsymbol w)}\\quad L(w_0,\\boldsymbol w)+\\frac{1}{2}\\|\\boldsymbol w\\|_2^2\\qquad$  （ただし，$\\|\\boldsymbol w\\|_2^2=w_1^2+\\cdots+w_d^2$）\n",
        "- L2正則化項 $\\frac{1}{2}\\|\\boldsymbol w\\|_2^2$ の勾配は $\\boldsymbol w$ なので， $\\boldsymbol w$ の修正量 $\\mathit\\Delta\\boldsymbol w$ が $\\boldsymbol w$ だけ増える．\n",
        "- fit を一行変更するだけで済む．$\\boldsymbol w$ は `self.coef_` のこと．\n",
        "- [scikit-learn の最小化問題は `n` で除さない仕様なので](https://scikit-learn.org/stable/modules/linear_model.html#binary-case)，平均ロジスティック損失を採用している場合は `self.coef_ / n` だけ修正量を増やす．\n",
        "\n",
        "\n",
        "    3. L2正則化を導入したアルゴリズムでは，学習率を大きくすると，重みは大きくならないが，収束しなくなる．  \n",
        "       このことを確認し，収束しない理由を説明してください．\n",
        "\n",
        "（ここに回答を書いてください）\n",
        "\n"
      ],
      "metadata": {
        "id": "jVhtoi5sM0Wm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "お疲れさまでした．"
      ],
      "metadata": {
        "id": "-svKKyHE-Vcl"
      }
    }
  ]
}